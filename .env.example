# cp .env.example .env
# Edit your .env file with your own values
# Don't commit your .env file to git/push to GitHub!
# Don't modify/delete .env.example unless adding extensions to the project
# which require new variable to be added to the .env file

# ----------------------------------
# API CONFIG
# OPENAI_API_MODEL can be used instead
# Special values:
# human - use human as intermediary with custom LLMs
# llama - use llama.cpp with Llama, Alpaca, Vicuna, GPT4All, etc.
# ----------------------------------
LLM_MODEL=gpt-3.5-turbo
MAX_TOKENS=2000   # OpenAI: 2000 / 7B-Llama: 1000 works best

# OPENAI API CONFIG
OPENAI_API_KEY=
OPENAI_API_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.5

# LLAMA model config (parameters enhanced for small local running Llama's)
LLAMA_TEMPERATURE=0.8   # default: 0.2 (values between 0.8 to 0.9 seem to work best for 7B-Llama)
LLAMA_CONTEXT=2000      # 7B-Llama: 2000 (different factors are applied on top for e.g. result storage context, document context, internet context, etc.)
LLAMA_CTX_MAX=2048      # best value: 2048 / alternatively: 1024 (some Llama's as e.g. wizardML feature a context length of 2048, with 1024 especially prioritization agent reponse can get truncated)
LLAMA_THREADS_NUM=8     # default: 8
LLAMA_MODEL_PATH=

# ----------------------------------
# STORE CONFIG
# TABLE_NAME can be used instead
# ----------------------------------
RESULTS_STORE_NAME=babyagi-db  # for chromadb directory 'chroma' is used instead

# Pinecone config
# Uncomment and fill these to switch from local ChromaDB to Pinecone
# PINECONE_API_KEY=
# PINECONE_ENVIRONMENT=

# Weaviate config
# Uncomment and fill these to switch from local ChromaDB to Weaviate
# WEAVIATE_USE_EMBEDDED=true
# WEAVIATE_URL=
# WEAVIATE_API_KEY=

# ----------------------------------
# COOPERATIVE MODE CONFIG
# BABY_NAME can be used instead
# ----------------------------------
INSTANCE_NAME=BabyAGI
COOPERATIVE_MODE=none

# RUN CONFIG
OBJECTIVE=Solve world hunger.
# For backwards compatibility
# FIRST_TASK can be used instead of INITIAL_TASK
INITIAL_TASK=Develop a task list

# Extensions
# List additional extension .env files to load (except .env.example!)
DOTENV_EXTENSIONS=

# Set to true to enable command line args support
ENABLE_COMMAND_LINE_ARGS=false

# ----------------------------------
# Write output to file (Experimental)
# Intended for step-by-step creation of a summary, story, report, etc. for the objective
# TODO: Add report parsing, processing, update and summarization functionality
# ----------------------------------
ENABLE_REPORT_EXTENSION=false
REPORT_FILE=report.txt

# Action to be performed for the OBJECTIVE
# The description 'chapter by chapter' and 'create a draft' must be kept, the rest can be adapted
ACTION=Create a Report with the relevant information.

# Instruction for text output to report file
INSTRUCTION=Begin text for the Report with 'Report-Tag'.

# ----------------------------------
# Internet smart search extension (based on smart search from BabyCatAGI)
# SERPAPI, Google CSE or browser search possible (works also w/o any API key!)
# Fallback strategy for missing API key and API rate limit
# Summarization of web scraping results with OpenAI or Llama
# ----------------------------------
ENABLE_SEARCH_EXTENSION=true
ENABLE_WIKI_SEARCH=false         # default: false (use wikipedia API instead of internet search, if better suited for the task, or use alone w/o internet search)
GOOGLE_API_KEY=
GOOGLE_CSE_ID=
SERPAPI_API_KEY=

# Temperature and CTX value for embedding LLM
SUMMARY_TEMPERATURE=0.7     # OpenAI: 0.7
SUMMARY_CTX_MAX=1024        # default: 1024 (used for Llama only, adapt SUMMARY_CONTEXT below accordingly)

# Context and webpage scrape length limits for smart internet search
SUMMARY_CONTEXT=3000   # OpenAI: 3000 (value of 1000 works fine or 7B-Llama)
SCRAPE_LENGTH=5000     # OpenAI: 5000 (value of 3000 is a good trade-off for 7B-Llama)

# Search summary model path (default: leave empty for gpt-3.5-turbo)
SUMMARY_MODEL_PATH=

# ----------------------------------
# Document embedding extension using langchain, chromadb and local LLM, with additional features for persistent entity memory
# The document embedding functionality is from: https://github.com/imartinez/privateGPT.git
# Relevant content from file privateGPT.py has integrated in BabyAGI
# The file ingest.py has been slightly adapted only
# File scraper.py has been added for scraping a list of urls and store to files for use with ingest.py
# Many thanks to https://github.com/imartinez for the great work!
# ----------------------------------
ENABLE_DOCUMENT_EXTENSION=false         # default: false (use document embedding as additional context, has to be enabled for persistent entity memory)
ENABLE_WIKI_DOC=false                   # default: false (use wikipedia API as additional context)
ENABLE_DOC_UPDATE=true                  # default: true (write web scrape summary to file, can be enabled independently from ENABLE_DOCUMENT_EXTENSION)
ENABLE_STORE_UPDATE=false               # default: false (update document embeddings with extended task results on-the-fly, works independently, create vector store before enabling the parameter)

# Document store config
DOC_STORE_NAME=chroma-memory            # name for document vectorstore
DOC_SOURCE_PATH=source_documents        # path to source documents
SCRAPE_SOURCE_PATH=scrape_documents     # path to scrape documents

# Document embedding model config
EMBEDDINGS_CTX_MAX=1024                 # default: 1024
EMBEDDINGS_TEMPERATURE=0.8              # default: 0.8 (LlamaCpp) or 0.7 (GPT4All)
EMBEDDINGS_MODEL_TYPE=LlamaCpp          # LlamaCpp or GPT4All
EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2  # default: all-MiniLM-L6-v2
EMBEDDINGS_MODEL_PATH=/Add/your/model/path       # default: has to be set according to used Llama model

