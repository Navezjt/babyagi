# cp .env.example .env
# Edit your .env file with your own values
# Don't commit your .env file to git/push to GitHub!
# Don't modify/delete .env.example unless adding extensions to the project
# which require new variable to be added to the .env file

# ----------------------------------
# API CONFIG
# OPENAI_API_MODEL can be used instead
# Special values:
# human - use human as intermediary with custom LLMs
# llama - use llama.cpp with Llama, Alpaca, Vicuna, GPT4All, etc.
# ----------------------------------
LLM_MODEL=gpt-3.5-turbo
MAX_TOKENS=2000   # OpenAI: 2000 / 7B-Llama: 1000 works best

# OPENAI API CONFIG
OPENAI_API_KEY=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
OPENAI_API_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.5

# LLAMA model config (parameters enhanced for small local running Llama's)
LLAMA_TEMPERATURE=0.8   # default: 0.2 (values between 0.8 to 0.9 seem to work best for 7B-Llama)
LLAMA_CONTEXT=2000      # 7B-Llama: 1000 to 2000 (different factors are applied on top for e.g. result storage context, embedding context, internet context, etc.)
LLAMA_CTX_MAX=2048      # best value: 2048 / alternatively: 1024 (some Llama's as e.g. wizardML feature a context length of 2048, with 1024 especially prioritization agent reponse can get truncated)
LLAMA_THREADS_NUM=8     # default: 8
LLAMA_MODEL_PATH=

# ----------------------------------
# STORE CONFIG
# TABLE_NAME can be used instead
# ----------------------------------
RESULTS_STORE_NAME=babyagi-db  # for chromadb directory 'chroma' is used instead

# Pinecone config
# Uncomment and fill these to switch from local ChromaDB to Pinecone
# PINECONE_API_KEY=
# PINECONE_ENVIRONMENT=

# Weaviate config
# Uncomment and fill these to switch from local ChromaDB to Weaviate
# WEAVIATE_USE_EMBEDDED=true
# WEAVIATE_URL=
# WEAVIATE_API_KEY=

# ----------------------------------
# COOPERATIVE MODE CONFIG
# BABY_NAME can be used instead
# ----------------------------------
INSTANCE_NAME=BabyAGI
COOPERATIVE_MODE=none

# RUN CONFIG
OBJECTIVE=Develop a python script for fine-tuning a Llama model on a consumer grade PC. Assume that all required libraries are installed and do not investigate how to install and setup software, hardware or tools.

#Research fine-tuning of Llama models and quantization methods to carry-out the fine-tuning on customer grade GPU.

#Research the history of Madrid, from its founding to the present day. Create a timeline of the most important events in the history of Madrid.

# For backwards compatibility
# FIRST_TASK can be used instead of INITIAL_TASK
INITIAL_TASK=Develop a task list
#Research the background information required to achieve the objective and create the python script.

# Extensions
# List additional extension .env files to load (except .env.example!)
DOTENV_EXTENSIONS=

# Set to true to enable command line args support
ENABLE_COMMAND_LINE_ARGS=false

# ----------------------------------
# Write output to file (Experimental)
# Intended for step-by-step creation of a summary, story, report, etc. for the objective
# TODO: Add report parsing, processing, update and summarization functionality
# ----------------------------------
ENABLE_REPORT_EXTENSION=false
REPORT_FILE=report.txt

# Action to be performed for the OBJECTIVE, e.g. create a report, story or code
# Just change the part "python script" accordingly. The statement "code block" makes the LLM start/end the output with "‘‘‘" (triple backticks), so no INSTRUCTION is required for marking the output.
ACTION=Output the python script in a code block.

# Instruction for marking of text output in task result (optional)
INSTRUCTION=

# ----------------------------------
# Internet smart search extension (based on smart search from BabyCatAGI)
# SERPAPI, Google CSE or browser search possible (works also w/o any API key!)
# Fallback strategy for missing API key and API rate limit
# Summarization of web scraping results with OpenAI or Llama
# ----------------------------------
ENABLE_SEARCH_EXTENSION=true
WIKI_SEARCH=false         # default: false (Use wikipedia API instead of internet search, if better suited for the task, or use alone w/o internet search)
GOOGLE_API_KEY=
GOOGLE_CSE_ID=
SERPAPI_API_KEY=

# Temperature and CTX value for embedding LLM
SUMMARY_TEMPERATURE=0.7     # OpenAI: 0.7
SUMMARY_CTX_MAX=1024        # default: 1024 (Used for Llama only, adapt SUMMARY_CONTEXT below accordingly)

# Context and webpage scrape length limits for smart internet search
SUMMARY_CONTEXT=3000   # OpenAI: 3000 (value of 1000 works fine or 7B-Llama)
SCRAPE_LENGTH=5000     # OpenAI: 5000 (value of 3000 is a good trade-off for 7B-Llama)

# Search summary model path (default: leave empty for gpt-3.5-turbo)
SUMMARY_MODEL_PATH=

# ----------------------------------
# Document embedding extension using langchain, chromadb and local LLM, with additional features for persistent entity memory
# The document embedding functionality is from: https://github.com/imartinez/privateGPT.git
# Many thanks to https://github.com/imartinez for the great work!
# Relevant content from file privateGPT.py has integrated in BabyAGI
# The file ingest.py has been slightly adapted only
# File scraper.py has been added for scraping a list of urls and store page content to files for use with ingest.py
# The sub-features WIKI_CONTEXT, MEMORY_BACKUP and MEMORY_UPDATE can be enabled independently from ENABLE_DOCUMENT_EXTENSION
# ----------------------------------
ENABLE_DOCUMENT_EXTENSION=false     # default: false (The vectorstore has to be created before enabling the parameter!)
WIKI_CONTEXT=false                  # default: false (Use wikipedia API as additional context)
MEMORY_BACKUP=true                  # default: true (Write enriched task results to file as "memory" backup)
MEMORY_UPDATE=true                  # default: true (Update document embeddings with enriched task results each cycle)

# Document store config
DOC_STORE_NAME=chroma-memory            # name for document vectorstore
DOC_SOURCE_PATH=source_documents        # path to source documents
SCRAPE_SOURCE_PATH=scrape_documents     # path to scrape documents

# Document embedding model config
EMBEDDINGS_CTX_MAX=1024                 # default: 1024
EMBEDDINGS_TEMPERATURE=0.8              # default: 0.8 (LlamaCpp) or 0.7 (GPT4All)
EMBEDDINGS_MODEL_TYPE=LlamaCpp          # LlamaCpp or GPT4All
EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2  # default: all-MiniLM-L6-v2
EMBEDDINGS_MODEL_PATH=/Add/model/path       # default: has to be set according to used Llama model

